{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":396802,"sourceType":"datasetVersion","datasetId":175990}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfolder_path = \"/kaggle/input/fashion-product-images-small/images\"\nfile_names = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\nprint(len(file_names))","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:03:00.809838Z","iopub.execute_input":"2024-01-04T06:03:00.810219Z","iopub.status.idle":"2024-01-04T06:05:15.706285Z","shell.execute_reply.started":"2024-01-04T06:03:00.810183Z","shell.execute_reply":"2024-01-04T06:05:15.705332Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"44441\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nprint(torch.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:15.708299Z","iopub.execute_input":"2024-01-04T06:05:15.708573Z","iopub.status.idle":"2024-01-04T06:05:15.713586Z","shell.execute_reply.started":"2024-01-04T06:05:15.708550Z","shell.execute_reply":"2024-01-04T06:05:15.712636Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"2.0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(file_names[0]))","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:15.714547Z","iopub.execute_input":"2024-01-04T06:05:15.714848Z","iopub.status.idle":"2024-01-04T06:05:15.725458Z","shell.execute_reply.started":"2024-01-04T06:05:15.714825Z","shell.execute_reply":"2024-01-04T06:05:15.724599Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'str'>\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\ndf = pd.read_csv(\"/kaggle/input/fashion-product-images-small/styles.csv\", on_bad_lines=\"skip\")\ncurrent = []\nfor x in df.id.values:\n    if str(x)+\".jpg\" not in file_names:\n        current.append(False)\n    else:\n        current.append(True)\ndf[\"Good\"] = np.array(current)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:15.726829Z","iopub.execute_input":"2024-01-04T06:05:15.727165Z","iopub.status.idle":"2024-01-04T06:05:42.481800Z","shell.execute_reply.started":"2024-01-04T06:05:15.727136Z","shell.execute_reply":"2024-01-04T06:05:42.480776Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"      id gender masterCategory subCategory  articleType baseColour  season  \\\n0  15970    Men        Apparel     Topwear       Shirts  Navy Blue    Fall   \n1  39386    Men        Apparel  Bottomwear        Jeans       Blue  Summer   \n2  59263  Women    Accessories     Watches      Watches     Silver  Winter   \n3  21379    Men        Apparel  Bottomwear  Track Pants      Black    Fall   \n4  53759    Men        Apparel     Topwear      Tshirts       Grey  Summer   \n\n     year   usage                             productDisplayName  Good  \n0  2011.0  Casual               Turtle Check Men Navy Blue Shirt  True  \n1  2012.0  Casual             Peter England Men Party Blue Jeans  True  \n2  2016.0  Casual                       Titan Women Silver Watch  True  \n3  2011.0  Casual  Manchester United Men Solid Black Track Pants  True  \n4  2012.0  Casual                          Puma Men Grey T-shirt  True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>gender</th>\n      <th>masterCategory</th>\n      <th>subCategory</th>\n      <th>articleType</th>\n      <th>baseColour</th>\n      <th>season</th>\n      <th>year</th>\n      <th>usage</th>\n      <th>productDisplayName</th>\n      <th>Good</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>15970</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Shirts</td>\n      <td>Navy Blue</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Turtle Check Men Navy Blue Shirt</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>39386</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Bottomwear</td>\n      <td>Jeans</td>\n      <td>Blue</td>\n      <td>Summer</td>\n      <td>2012.0</td>\n      <td>Casual</td>\n      <td>Peter England Men Party Blue Jeans</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59263</td>\n      <td>Women</td>\n      <td>Accessories</td>\n      <td>Watches</td>\n      <td>Watches</td>\n      <td>Silver</td>\n      <td>Winter</td>\n      <td>2016.0</td>\n      <td>Casual</td>\n      <td>Titan Women Silver Watch</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>21379</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Bottomwear</td>\n      <td>Track Pants</td>\n      <td>Black</td>\n      <td>Fall</td>\n      <td>2011.0</td>\n      <td>Casual</td>\n      <td>Manchester United Men Solid Black Track Pants</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>53759</td>\n      <td>Men</td>\n      <td>Apparel</td>\n      <td>Topwear</td>\n      <td>Tshirts</td>\n      <td>Grey</td>\n      <td>Summer</td>\n      <td>2012.0</td>\n      <td>Casual</td>\n      <td>Puma Men Grey T-shirt</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df[\"Good\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:42.484509Z","iopub.execute_input":"2024-01-04T06:05:42.484858Z","iopub.status.idle":"2024-01-04T06:05:42.503302Z","shell.execute_reply.started":"2024-01-04T06:05:42.484830Z","shell.execute_reply":"2024-01-04T06:05:42.502430Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"Good\nTrue     44419\nFalse        5\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df[\"masterCategory\"].unique()","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:46:04.299465Z","iopub.execute_input":"2024-01-04T06:46:04.300176Z","iopub.status.idle":"2024-01-04T06:46:04.309765Z","shell.execute_reply.started":"2024-01-04T06:46:04.300145Z","shell.execute_reply":"2024-01-04T06:46:04.308800Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"array(['Apparel', 'Accessories', 'Footwear', 'Personal Care',\n       'Free Items', 'Sporting Goods', 'Home'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"idx = []\ncategory = []\ncolor = []\nfor a, b, c, d in zip(df.id.values, df.masterCategory.values, df.baseColour.values, df.Good.values):\n    if d:\n        idx.append(a)\n        category.append(b)\n        color.append(c)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:42.504321Z","iopub.execute_input":"2024-01-04T06:05:42.504608Z","iopub.status.idle":"2024-01-04T06:05:42.546728Z","shell.execute_reply.started":"2024-01-04T06:05:42.504577Z","shell.execute_reply":"2024-01-04T06:05:42.546037Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame()\ndata[\"id\"] = np.array(idx)\ndata[\"masterCategory\"] = np.array(category)\ndata[\"baseColour\"] = np.array(color)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:42.548109Z","iopub.execute_input":"2024-01-04T06:05:42.548641Z","iopub.status.idle":"2024-01-04T06:05:42.606926Z","shell.execute_reply.started":"2024-01-04T06:05:42.548608Z","shell.execute_reply":"2024-01-04T06:05:42.606179Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"data.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:42.608060Z","iopub.execute_input":"2024-01-04T06:05:42.608863Z","iopub.status.idle":"2024-01-04T06:05:42.615037Z","shell.execute_reply.started":"2024-01-04T06:05:42.608828Z","shell.execute_reply":"2024-01-04T06:05:42.614034Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(44419, 3)"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim \nimport torchvision.transforms as transforms\nimport torchvision\nimport os\nimport pandas as pd\nfrom PIL import Image\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n)\n\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to the same size\n    transforms.ToTensor(),\n])\n\nclass CustomDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None):\n        self.annotations = data\n        self.preprocess = {}\n        for x, y in enumerate(self.annotations[\"masterCategory\"].unique()):\n            self.preprocess[y] = x\n        self.annotations[\"masterCategory\"] = self.annotations[\"masterCategory\"].map(self.preprocess)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, index):\n        img_path = os.path.join(self.root_dir, str(self.annotations.iloc[index, 0])+\".jpg\")\n        image = Image.open(img_path).convert(\"RGB\")\n        y_label = torch.tensor(int(self.annotations.iloc[index, 1]))\n        if self.transform:\n            image = self.transform(image)\n        return (image, y_label)\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nin_channel = 3\nnum_classes = 7\nlearning_rate = 3e-4\nbatch_size = 32\nnum_epochs = 5\n\n# Load Data\ndataset = CustomDataset(\n    csv_file=\"/kaggle/input/fashion-product-images-small/styles.csv\",\n    root_dir=\"/kaggle/input/fashion-product-images-small/images\",\n    transform=transform,\n)\n\n\ntrain_set, test_set = torch.utils.data.random_split(dataset, [35000, 9419])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\n# Model\nmodel = torchvision.models.googlenet(weights=\"DEFAULT\")\n\nfor param in model.parameters():\n    param.requires_grad = False\n\n# final layer is not frozen\nmodel.fc = nn.Linear(in_features=1024, out_features=num_classes)\nmodel.to(device)\n\n# Loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:42.616540Z","iopub.execute_input":"2024-01-04T06:05:42.616808Z","iopub.status.idle":"2024-01-04T06:05:46.643825Z","shell.execute_reply.started":"2024-01-04T06:05:42.616785Z","shell.execute_reply":"2024-01-04T06:05:46.643034Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/googlenet-1378be20.pth\" to /root/.cache/torch/hub/checkpoints/googlenet-1378be20.pth\n100%|██████████| 49.7M/49.7M [00:00<00:00, 289MB/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"for epoch in range(num_epochs):\n    losses = []\n\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # Get data to cuda if possible\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        losses.append(loss.item())\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n    print(f\"Cost at epoch {epoch} is {sum(losses)/len(losses)}\")\n\n# Check accuracy on training to see how good our model is\ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n        print(\n            f\"Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}\"\n        )\n\n    model.train()\n\nprint(\"Checking accuracy on Training Set\")\ncheck_accuracy(train_loader, model)\n\nprint(\"Checking accuracy on Test Set\")\ncheck_accuracy(test_loader, model)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:05:46.644967Z","iopub.execute_input":"2024-01-04T06:05:46.645256Z","iopub.status.idle":"2024-01-04T06:20:47.091028Z","shell.execute_reply.started":"2024-01-04T06:05:46.645232Z","shell.execute_reply":"2024-01-04T06:20:47.090127Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Cost at epoch 0 is 0.36802011933960904\nCost at epoch 1 is 0.19328657658923915\nCost at epoch 2 is 0.17048388668140174\nCost at epoch 3 is 0.15545034706013675\nCost at epoch 4 is 0.14818202853764678\nChecking accuracy on Training Set\nGot 33976 / 35000 with accuracy 97.07\nChecking accuracy on Test Set\nGot 9129 / 9419 with accuracy 96.92\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(model, 'googlenet_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:20:47.092323Z","iopub.execute_input":"2024-01-04T06:20:47.092701Z","iopub.status.idle":"2024-01-04T06:20:47.176679Z","shell.execute_reply.started":"2024-01-04T06:20:47.092667Z","shell.execute_reply":"2024-01-04T06:20:47.175963Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# predictions\nimport torch\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet50\nfrom PIL import Image\nimport torchvision.models as models\n\nloaded_model = torch.load(\"/kaggle/working/googlenet_model.pth\")\nloaded_model.eval()\n\n# Define the image preprocessing function\ndef preprocess_image(image_path):\n    image = Image.open(image_path).convert('RGB')\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n    input_tensor = transform(image)\n    input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n    return input_batch\n\n# Define the prediction function\ndef predict_image(model, image_path):\n    input_tensor = preprocess_image(image_path)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    input_tensor = input_tensor.to(device)\n    model = model.to(device)\n    with torch.no_grad():\n        output = model(input_tensor)\n\n    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n    return probabilities\n\n# Specify the path to the new image\nnew_image_path = '/kaggle/input/fashion-product-images-small/images/34123.jpg'\n\n# Make predictions for the new image\nprobabilities = predict_image(loaded_model, new_image_path)\n\n# Display the predicted probabilities for each class\nprint(probabilities)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:41:28.343087Z","iopub.execute_input":"2024-01-04T06:41:28.343461Z","iopub.status.idle":"2024-01-04T06:41:28.444554Z","shell.execute_reply.started":"2024-01-04T06:41:28.343432Z","shell.execute_reply":"2024-01-04T06:41:28.443511Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"tensor([1.6573e-03, 7.1084e-01, 1.7832e-02, 2.6795e-01, 1.1039e-03, 6.1932e-04,\n        3.3632e-06], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"cat = ['Apparel', 'Accessories', 'Footwear', 'Personal Care',\n       'Free Items', 'Sporting Goods', 'Home']\nmapping = {}\nfor x, y in enumerate(cat):\n    mapping[y] = x\nprint(mapping)","metadata":{"execution":{"iopub.status.busy":"2024-01-04T06:47:40.037749Z","iopub.execute_input":"2024-01-04T06:47:40.038110Z","iopub.status.idle":"2024-01-04T06:47:40.044477Z","shell.execute_reply.started":"2024-01-04T06:47:40.038082Z","shell.execute_reply":"2024-01-04T06:47:40.043444Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"{'Apparel': 0, 'Accessories': 1, 'Footwear': 2, 'Personal Care': 3, 'Free Items': 4, 'Sporting Goods': 5, 'Home': 6}\n","output_type":"stream"}]}]}